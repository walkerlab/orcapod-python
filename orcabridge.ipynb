{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I define few critical concepts for Orcapod.\n",
    "\n",
    "* `Data` -- In Orcapod, smallest unit of `data` is a single `file`. Unlike many other computation pipeline system, Orcapod pipeline in principle does **not** operate on `data` that's not a file. In other words, `Oracpod` pipeline will **not** pass a data in memory from one node to another. Consequently, all operations and processing in Orcapod pipeline revolves around `file` (NOTE: this is a particularly strong/restrictive version of Oracpod pipeline. We may consider extending data to things like environment variable and command line arguments)\n",
    "* `Pathset` -- a unit of data that can be passed into a pod. A `pathset` consists of a file, a directory, or a collection of one or more file and directories.\n",
    "* `Packet` -- a single concrete instance of key-value pair, mapping packet key to a single `pathset`.\n",
    "* `Stream` -- a series of one or more `packets` flowing from a `data producer` to a `data consumer`. In a directed acyclic graph represneing an Orcapod `pipeline`, a `stream` corresponds to a *directed* edge connecting from a data source into a `data consumer` (e.g., `pod`)\n",
    "* `Data producer` and `data consumer` -- in the Orcapod data pipeline, data (in form of `packet` of data flowing inside a `stream`) flows from a `data producer` to a `data consumer`. Consequentially, a `data consumer` may in turn act as a `data producer` downstream\n",
    "* `Data source` -- Root level `data producer` (that is, the data originates from this `data producer` and it is not a `data consumer` of any stream). Typically `data source` is tied to a data storage, although you could have *procedural* `data source` where data packets are produced programatically.\n",
    "* `Tag` -- each `Packet` in a stream *may be* associated with a `tag` that helps to assign semantic identity to the particular `packet`. For example, a data `packet` for an experimental data may be associated with a `tag` of session ID. Note that while `tag` provides a convenient and often meaningful ways of identifying and referring to specific packet within a stream, it should **not** be considered to be the defining identity of the `packet`. Identity of the `packet` is strictly determined by the exact data content of the `packet`, and not by how you refer to it. Consequently, it may be that two packets with an identical content (and thus shared identity) are associated with distinct `tags` in a `stream`. Conversely, an identical `tag` may be associated with two distinct `packets` in a stream.  Typically, one would associate a unique `tag` for each packet in the stream.\n",
    "* `Operation` -- A *node* in the directed acyclic graph representing an Orcapod `pipeline`, corresponding to a step of data processing/transformation/computation. An `Operation` receives can be classified into either a `mapper` or a `pod` based on their role in `data provenance`.\n",
    "* `Mapper` -- A class of `operation` that does **not** result in creation/alteration of a new data -- that is, `operation` does **not** every create or modify a file *content*. More specifically, `Mapper` operation can not produce a path that was not already present in the input streams to the `mapper`. This feature ensures that a `mapper` is fundamentally not involved in the reproducibility of computation. Consequently, `mapper` information is not necessary for the maintenance of proper `data provenance` in a tree of computation. However, `mapper` plays critical role in the actual execution of a data pipeline, determining which data `packet` will be fed into operations in the pipeline directed acyclic graph (DAG). Note that as long as it doesn't modify the content of any file, a `mapper` may inspect the content of any file in a `packet` it receives and alter its behavior based on the content of the file. In other words, `mapper` may alter what data file(s) gets passed around without changing/creating any file based on a rule that depends on `tag`, `packet` key (`argument` name) and/or file content.\n",
    "* `Pod` (e.g. FunctionPod) -- fundamental unit of computation in Orcapod. `Pod` is the only class of `operation` that may create a new file. Critically, when operating within an Orcapod `pipeline`, a `pod` will **not** receive the `tag` information. Rather, `pod` must strictly operate on a single `packet`. An ideal `pod` will have completely deterministic behavior that only depends on the `packet` identity (that is, packet keys and `pathset` contents)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Orcabrdige?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Orcabridge` provide prototypal implementation of the above-defined key concepts in `orcapod`, with particular focus given to `stream`, `packet`, `tag`, `operation` (`pod` and `mapper`). This package provides the reference implementation of both synchronous and asynchronous `streams` as a sequence of `packets` associated with a `tag`. \n",
    "\n",
    "### Key limitations\n",
    "Being a very basic reference implementation with a goal of serving as a playground for conceptual and algorithmic development, there are a few notable limitations in `orcabridge`, lacking some important features that would be found in the full implementation of `orcapod`.\n",
    "\n",
    "* Limited `pathset` -- Only a very rudimentary implementation of a `pathset` can be found, where a typical `packet` would be a simple key-value pair where the value is a single file path. \n",
    "* `FunctionPod` in place of `pod` -- A ideal `pod` will represent a completely reproducible unit of computation with strict dependence on the input `packet` to the `pod`. However, in `orcabridge` is equivalated to a Python function -- even more specifically, the name assigned to the Python function. In otherwords, two `FunctionPods` are considered identical if they share the same function name. Quite obviously this is a gross simplification that can be trivially violated. However, for the intended purpose of `orcabridge`, this rather simplistic implementation/definition of `pod` should suffice to test all features in the `pipeline`. Consequently, to ensure that the pipeline operates as expected, **do not use the same name for two distinct functions**. Doing so will break the fundamental assumption of `FunctionPod` and lead to completely erradic behavior. On the other hand, it is ok to have one or more name used to refer to an identical function.\n",
    "* Different pipeline DAG defintion -- In `orcapod` the directed acyclic graph (DAG) for the `pipeline` should be defined using YAML file (or less frequently using API on `pipeline` struct in the Rust library). In `orcabridge` you will find that a `pipeline` DAG is defined dynamically through a series of application of `operation`. This is very much akin to how some DAG-based neural network library like TensorFlow defines a computation graph. While this works well for simple examples, it is rather difficult to track changes to the pipeline defined dynamically/programmatically using version control system. Since *how* you define the pipeline DAG is strictly speaking an orthogonal problem to the everything else that concerns the operation of the `pipeline`, no effort will be given to align the DAG definition in `orcabridge` and `orcapod`.\n",
    "* Limited usage of a `stream` -- Currently `orcabrdige` only support single producer single consumer (SCSP) `stream`x, whereas in `orcapod`, `stream` should support single producer multiple consumer (SPMC) paradigm. While the same stream can be used in multiple downstream operations, each iteration of the stream actually results in recomputations of the entire chain of pipeline leading up to that stream. This inefficiency can be ameliorated by `CacheStream` operation after particularly computationally expensive segment of the pipeline. Using storage-backed `FunctionPod` will also help ameliorate the cost of recomputation by retrieving memoized computation result instead of recomputing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Orcabridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we explore the usage of `orcabridge` package, enumerating the core components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Stream` is fundamental to Orcapod datapipeline, representing *edges* in a directed acyclic graph (DAG) of an Orcapod pipeline. `Stream` is best thought of as a flowing stream of `packets` -- a unit of data in Oracpod. A `packet` is essentially a ditionary mapping argument names to a `pathset` (that is, one or more files with arbitrary nesting). Ultimately, a pod will receive and work on the `packet`, looking up the pathset that matches the expected argument names defined as the inputs into the pod. Before we explore creating and using `pod`, we will create a very basic `stream` called `GlobStream`, sourcing from a directory. A packet is formed for each file that matches the specified *glob* pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orcabridge.source import GlobSource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a data source out of all `*.txt` files found in the folder `examples/dataset1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mday1.txt\u001b[0m*  \u001b[01;32mday2.txt\u001b[0m*  \u001b[01;32mday3.txt\u001b[0m*  \u001b[01;32mday4.txt\u001b[0m*\n"
     ]
    }
   ],
   "source": [
    "ls ./examples/dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = GlobSource('txt_file', './examples/dataset1', '*.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then obtain `stream` from a `source` by invoking the source with `Source()`. The return `stream` acts as an iterator over the `packet` and its `tag`.\n",
    "For convenience, `source` can also directly act as an iterator, equivalent to iterating over a stream returned by calling the source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packet {'txt_file': PosixPath('examples/dataset1/day1.txt')} with tag {'file_name': 'day1'}\n",
      "Packet {'txt_file': PosixPath('examples/dataset1/day2.txt')} with tag {'file_name': 'day2'}\n",
      "Packet {'txt_file': PosixPath('examples/dataset1/day3.txt')} with tag {'file_name': 'day3'}\n",
      "Packet {'txt_file': PosixPath('examples/dataset1/day4.txt')} with tag {'file_name': 'day4'}\n"
     ]
    }
   ],
   "source": [
    "for tag, packet in dataset1():\n",
    "    print(f'Packet {packet} with tag {tag}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packet {'txt_file': PosixPath('examples/dataset1/day1.txt')} with tag {'file_name': 'day1'}\n",
      "Packet {'txt_file': PosixPath('examples/dataset1/day2.txt')} with tag {'file_name': 'day2'}\n",
      "Packet {'txt_file': PosixPath('examples/dataset1/day3.txt')} with tag {'file_name': 'day3'}\n",
      "Packet {'txt_file': PosixPath('examples/dataset1/day4.txt')} with tag {'file_name': 'day4'}\n"
     ]
    }
   ],
   "source": [
    "# equivalent to above but more natural without the need to call `dataset1()`\n",
    "for tag, packet in dataset1:\n",
    "    print(f'Packet {packet} with tag {tag}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to note. When creating the `GlobSource` we pass in the argument name to be associated with the `pathset` matching our glob pattern (`*.txt` in this case). By default, the `GlobSource` tags each packet with a key of `file_name` and value of the name of the file that was matched (minus the file extension). This behavior can be easily changed by passing in a custom function for tag generation at the time of `GlobSource` creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "dataset1_custom = GlobSource('data', './examples/dataset1', '*.txt', tag_function=lambda x: {'date': Path(x).stem})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packet {'data': PosixPath('examples/dataset1/day1.txt')} with tag {'date': 'day1'}\n",
      "Packet {'data': PosixPath('examples/dataset1/day2.txt')} with tag {'date': 'day2'}\n",
      "Packet {'data': PosixPath('examples/dataset1/day3.txt')} with tag {'date': 'day3'}\n",
      "Packet {'data': PosixPath('examples/dataset1/day4.txt')} with tag {'date': 'day4'}\n"
     ]
    }
   ],
   "source": [
    "for tag, packet in dataset1_custom:\n",
    "    print(f'Packet {packet} with tag {tag}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom tag function would allow one to extract information useful in controlling the flow of the data pipeline from the file path or even the file content. We will return to this a bit later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, a packet is generated and starts flowing into a `stream` **only** when you ask for it by iterating through the elements. This allows for a series of streams and pods to be chained together without immediately invoking any computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and load another source from a folder containing multiple `*.bin` files, representing data collected on different days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packet {'bin_data': PosixPath('examples/dataset2/session_day1.bin')} with tag {'file_name': 'session_day1'}\n",
      "Packet {'bin_data': PosixPath('examples/dataset2/session_day3.bin')} with tag {'file_name': 'session_day3'}\n",
      "Packet {'bin_data': PosixPath('examples/dataset2/session_day4.bin')} with tag {'file_name': 'session_day4'}\n",
      "Packet {'bin_data': PosixPath('examples/dataset2/session_day5.bin')} with tag {'file_name': 'session_day5'}\n"
     ]
    }
   ],
   "source": [
    "dataset2 = GlobSource('bin_data', './examples/dataset2', '*.bin')\n",
    "\n",
    "for tag, packet in dataset2:\n",
    "    print(f'Packet {packet} with tag {tag}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two streams to work with, let's explore how we can manipulate/control the flow of streams using `operations` and, specifically, `mapper` operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating streams with `operations`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As defined ealier in the [core concepts](#core-concepts), we refer to any computation/transformation that works on stream(s) as `operations` in the pipeline. If the Orcapod pipeline were to be viewed as a DAG, the `streams` would be the edges connecting *nodes* that are the `operations`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Operations` can be divided into two categories based on their roles in the processing and manipulating streams. `Mappers` are `operations` that controls and alter the streams but *without generating or modifying new data files*. As we will see shortly, `mappers` work to alter the stream by alterning packet tags and/or packet content, but critically will never create or modify new files that were not already present somewhere in the stream feeding into the `mapper` node. While this might sound like an unnecessary restriction on what `mappers` can do, we will see that this property guarantees that *mappers can not ever alter the reproducibility of computational chains*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second category of `operations` are called the `pods`, and as you may have guessed, these operations are **allowed to generate and flow new files into the streams**. In fact, `pods` are the only operations that can introduce new files into the stream. Consequently, stream `sources` such as `GlobFileSource` we saw earlier are also a type of pods, that's special in that it takes no input streams!\n",
    "\n",
    "We will explore pods in great detail later. First let's get to know `mappers`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controling data streams with `Mappers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have created `source` from which streams can be formed, you can alter the stream by applying various `mappers`. More precisely, a `mapper` can work on tags and/or packets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map keys\n",
    "Likey one of the most common mapper operation to be found in Orcapod pipeline is `MapKeys` mapper. As the name implies, it let's you alter the keys (argument names) found in the `packet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before mapping:\n",
      "Packet {'txt_file': PosixPath('examples/dataset1/day1.txt')} with tag {'file_name': 'day1'}\n",
      "Packet {'txt_file': PosixPath('examples/dataset1/day2.txt')} with tag {'file_name': 'day2'}\n",
      "Packet {'txt_file': PosixPath('examples/dataset1/day3.txt')} with tag {'file_name': 'day3'}\n",
      "Packet {'txt_file': PosixPath('examples/dataset1/day4.txt')} with tag {'file_name': 'day4'}\n",
      "After mapping:\n",
      "Mapped Packet {'content': PosixPath('examples/dataset1/day1.txt')} with tag {'file_name': 'day1'}\n",
      "Mapped Packet {'content': PosixPath('examples/dataset1/day2.txt')} with tag {'file_name': 'day2'}\n",
      "Mapped Packet {'content': PosixPath('examples/dataset1/day3.txt')} with tag {'file_name': 'day3'}\n",
      "Mapped Packet {'content': PosixPath('examples/dataset1/day4.txt')} with tag {'file_name': 'day4'}\n"
     ]
    }
   ],
   "source": [
    "from orcabridge.mapper import MapKeys\n",
    "\n",
    "print(\"Before mapping:\")\n",
    "for tag, packet in dataset1:\n",
    "    print(f'Packet {packet} with tag {tag}')\n",
    "\n",
    "\n",
    "# create a new stream mapping packet keys 'txt_file' to 'content'\n",
    "key_mapper = MapKeys(key_map={'txt_file': 'content'})\n",
    "\n",
    "print(\"After mapping:\")\n",
    "for tag, packet in key_mapper(dataset1):\n",
    "    print(f'Mapped Packet {packet} with tag {tag}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'd notice that for each packet, the key `txt_file` was replaced with `content` without altering the pointed `path` or the associated tag. As the keys of the packets will be used as the name of arguments when invoking pods on a stream, we will see that `MapKeys` are commonly used to *map* the correct path to the argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map tags\n",
    "As we have already seen, each packet in the stream is associated with a tag, often derived from the data source. In the case of `GlobFileSource`, the tags are by default the name of the file that formed the packet. These tags are used to *transiently* identify the packet and will be used when matching packets across multiple streams (as we will see shortly in `Join` operation). You can manipulate the tags using `MapTags` operation, much like `MapKeys` but operating on the tags for each packaet under a uniform renaming rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'day': 'day1'} {'txt_file': PosixPath('examples/dataset1/day1.txt')}\n",
      "{'day': 'day2'} {'txt_file': PosixPath('examples/dataset1/day2.txt')}\n",
      "{'day': 'day3'} {'txt_file': PosixPath('examples/dataset1/day3.txt')}\n",
      "{'day': 'day4'} {'txt_file': PosixPath('examples/dataset1/day4.txt')}\n"
     ]
    }
   ],
   "source": [
    "from orcabridge.mapper import MapTags\n",
    "tag_mapper = MapTags(tag_map={'file_name': 'day'})\n",
    "\n",
    "for tag, packet in tag_mapper(dataset1):\n",
    "    print(tag, packet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might expect, you can chain multiple operations one after another to construct a more complex stream. Below, we first apply the key mapping and then map tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped Packet {'content': PosixPath('examples/dataset1/day1.txt')} with tag {'day': 'day1'}\n",
      "Mapped Packet {'content': PosixPath('examples/dataset1/day2.txt')} with tag {'day': 'day2'}\n",
      "Mapped Packet {'content': PosixPath('examples/dataset1/day3.txt')} with tag {'day': 'day3'}\n",
      "Mapped Packet {'content': PosixPath('examples/dataset1/day4.txt')} with tag {'day': 'day4'}\n"
     ]
    }
   ],
   "source": [
    "key_mapper = MapKeys(key_map={'txt_file': 'content'})\n",
    "key_mapped_stream = key_mapper(dataset1)\n",
    "\n",
    "tag_mapper = MapTags(tag_map={'file_name': 'day'})\n",
    "tag_and_key_mapped = tag_mapper(key_mapped_stream)\n",
    "\n",
    "for tag, packet in tag_and_key_mapped:\n",
    "    print(f\"Mapped Packet {packet} with tag {tag}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth emphasizing again that all computations are triggered only when you iterate through the final stream `tag_and_key_mapped`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although not recommended as it reduces readability, you can create and immediately apply `mapper` to achieve the same processing in a fewer lines of code (albeit, with worse readability):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped Packet {'content': PosixPath('examples/dataset1/day1.txt')} with tag {'day': 'day1'}\n",
      "Mapped Packet {'content': PosixPath('examples/dataset1/day2.txt')} with tag {'day': 'day2'}\n",
      "Mapped Packet {'content': PosixPath('examples/dataset1/day3.txt')} with tag {'day': 'day3'}\n",
      "Mapped Packet {'content': PosixPath('examples/dataset1/day4.txt')} with tag {'day': 'day4'}\n"
     ]
    }
   ],
   "source": [
    "# totally valid, but difficult to read and thus not recommended\n",
    "for tag, packet in MapTags(tag_map={'file_name': 'day'})(MapKeys(key_map={'txt_file': 'content'})(dataset1)):\n",
    "    print(f\"Mapped Packet {packet} with tag {tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining multiple streams into a single stream\n",
    "Now that we have looked at how you can manipulate a single stream, let's turn our eyes to how you can work with more than one streams together.\n",
    "\n",
    "By the far the most common multi-stream operations will be to join two (or more) streams into a single, bigger stream. \n",
    "You can combine multiple streams into one by using `Join` operation, matching packets from each stream based on the matching tags. If tags from two streams have shared key, the value must be identical for all shared keys for the two packets to be matched. The matched packets are then merged into a one (typically larger) packet and shipped to the output stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if we join `dataset1` and `dataset2`, where:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1:\n",
      "Tag: {'file_name': 'day1'}, Packet: {'txt_file': PosixPath('examples/dataset1/day1.txt')}\n",
      "Tag: {'file_name': 'day2'}, Packet: {'txt_file': PosixPath('examples/dataset1/day2.txt')}\n",
      "Tag: {'file_name': 'day3'}, Packet: {'txt_file': PosixPath('examples/dataset1/day3.txt')}\n",
      "Tag: {'file_name': 'day4'}, Packet: {'txt_file': PosixPath('examples/dataset1/day4.txt')}\n",
      "\n",
      "Dataset 2:\n",
      "Tag: {'file_name': 'session_day1'}, Packet: {'bin_data': PosixPath('examples/dataset2/session_day1.bin')}\n",
      "Tag: {'file_name': 'session_day3'}, Packet: {'bin_data': PosixPath('examples/dataset2/session_day3.bin')}\n",
      "Tag: {'file_name': 'session_day4'}, Packet: {'bin_data': PosixPath('examples/dataset2/session_day4.bin')}\n",
      "Tag: {'file_name': 'session_day5'}, Packet: {'bin_data': PosixPath('examples/dataset2/session_day5.bin')}\n"
     ]
    }
   ],
   "source": [
    "# dataset 1\n",
    "print(\"Dataset 1:\")\n",
    "for tag, packet in dataset1:\n",
    "    print(f'Tag: {tag}, Packet: {packet}')\n",
    "\n",
    "# dataset 2\n",
    "print(\"\\nDataset 2:\")\n",
    "for tag, packet in dataset2:\n",
    "    print(f'Tag: {tag}, Packet: {packet}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any guess what would happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orcabridge.mapper import Join\n",
    "join_op = Join()\n",
    "\n",
    "for tag, packet in join_op(dataset1, dataset2):\n",
    "    print(f'Tag: {tag}, Packet: {packet}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be surprised to see that the joined stream is completely empty! This is because packets from both streams were tagged with key `file_name`, causing the `Join` to combine packets only if the value of `file_name` matches exactly. Since no filenames matched, the resulting stream was empty!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we can make use of the other `mappers` to our advantage and achieve more useful join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's completely rename the tag key for one of the streams and see what would happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 Tag: {'day': 'day1', 'file_name': 'session_day1'}, Packet: {'txt_file': PosixPath('examples/dataset1/day1.txt'), 'bin_data': PosixPath('examples/dataset2/session_day1.bin')}\n",
      "02 Tag: {'day': 'day1', 'file_name': 'session_day3'}, Packet: {'txt_file': PosixPath('examples/dataset1/day1.txt'), 'bin_data': PosixPath('examples/dataset2/session_day3.bin')}\n",
      "03 Tag: {'day': 'day1', 'file_name': 'session_day4'}, Packet: {'txt_file': PosixPath('examples/dataset1/day1.txt'), 'bin_data': PosixPath('examples/dataset2/session_day4.bin')}\n",
      "04 Tag: {'day': 'day1', 'file_name': 'session_day5'}, Packet: {'txt_file': PosixPath('examples/dataset1/day1.txt'), 'bin_data': PosixPath('examples/dataset2/session_day5.bin')}\n",
      "05 Tag: {'day': 'day2', 'file_name': 'session_day1'}, Packet: {'txt_file': PosixPath('examples/dataset1/day2.txt'), 'bin_data': PosixPath('examples/dataset2/session_day1.bin')}\n",
      "06 Tag: {'day': 'day2', 'file_name': 'session_day3'}, Packet: {'txt_file': PosixPath('examples/dataset1/day2.txt'), 'bin_data': PosixPath('examples/dataset2/session_day3.bin')}\n",
      "07 Tag: {'day': 'day2', 'file_name': 'session_day4'}, Packet: {'txt_file': PosixPath('examples/dataset1/day2.txt'), 'bin_data': PosixPath('examples/dataset2/session_day4.bin')}\n",
      "08 Tag: {'day': 'day2', 'file_name': 'session_day5'}, Packet: {'txt_file': PosixPath('examples/dataset1/day2.txt'), 'bin_data': PosixPath('examples/dataset2/session_day5.bin')}\n",
      "09 Tag: {'day': 'day3', 'file_name': 'session_day1'}, Packet: {'txt_file': PosixPath('examples/dataset1/day3.txt'), 'bin_data': PosixPath('examples/dataset2/session_day1.bin')}\n",
      "10 Tag: {'day': 'day3', 'file_name': 'session_day3'}, Packet: {'txt_file': PosixPath('examples/dataset1/day3.txt'), 'bin_data': PosixPath('examples/dataset2/session_day3.bin')}\n",
      "11 Tag: {'day': 'day3', 'file_name': 'session_day4'}, Packet: {'txt_file': PosixPath('examples/dataset1/day3.txt'), 'bin_data': PosixPath('examples/dataset2/session_day4.bin')}\n",
      "12 Tag: {'day': 'day3', 'file_name': 'session_day5'}, Packet: {'txt_file': PosixPath('examples/dataset1/day3.txt'), 'bin_data': PosixPath('examples/dataset2/session_day5.bin')}\n",
      "13 Tag: {'day': 'day4', 'file_name': 'session_day1'}, Packet: {'txt_file': PosixPath('examples/dataset1/day4.txt'), 'bin_data': PosixPath('examples/dataset2/session_day1.bin')}\n",
      "14 Tag: {'day': 'day4', 'file_name': 'session_day3'}, Packet: {'txt_file': PosixPath('examples/dataset1/day4.txt'), 'bin_data': PosixPath('examples/dataset2/session_day3.bin')}\n",
      "15 Tag: {'day': 'day4', 'file_name': 'session_day4'}, Packet: {'txt_file': PosixPath('examples/dataset1/day4.txt'), 'bin_data': PosixPath('examples/dataset2/session_day4.bin')}\n",
      "16 Tag: {'day': 'day4', 'file_name': 'session_day5'}, Packet: {'txt_file': PosixPath('examples/dataset1/day4.txt'), 'bin_data': PosixPath('examples/dataset2/session_day5.bin')}\n"
     ]
    }
   ],
   "source": [
    "dataset1_retagged = MapTags(tag_map={'file_name': 'day'})(dataset1)\n",
    "\n",
    "for i, (tag, packet)in enumerate(join_op(dataset1_retagged, dataset2)):\n",
    "    print(f'{i+1:02d} Tag: {tag}, Packet: {packet}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now getting something -- in fact, quite a few things. If you look carefully at the `packet`, you'll notice that it now contains two keys/arguments -- `txt_file` and `bin_data`, combining the packets from the two datasets. \n",
    "\n",
    "The `tags` also now contain two keys `day` from the re-tagged dataset1 stream and `file_name` from unchanged dataset2 stream.\n",
    "\n",
    "Since the two streams share no common tags, the `Join` operation results in *full-multiplexing* of two streams. With each stream containing 4 packets, you get $4 \\times 4 = 16$ packets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it is not all too useful if all `Join` can do is to produce either 0 packet or a full combination of packets from two streams. The true value of `Join` lies in its ability to match two packets that are *related* to each other. \n",
    "\n",
    "In our example datasets, you likely noticed that files from both datasets are associated with a day. Let's now try to join the two dataset streams by matching by the day!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we could achieve the desired effect by changing how we load the source, passing in custom `tag_function` into `GlobSource`, let's achieve the same by using another `mapper` called `Transform`. `Transform` effectively combines `MapKey` and `MapTag` but further allows you to provide a function that will receive the tag and packet, one at a time, and return a (potentially modified) tag and/or packet, achieving the desired transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag: {'day': 'day1'}, Packet: {'bin_data': PosixPath('examples/dataset2/session_day1.bin')}\n",
      "Tag: {'day': 'day3'}, Packet: {'bin_data': PosixPath('examples/dataset2/session_day3.bin')}\n",
      "Tag: {'day': 'day4'}, Packet: {'bin_data': PosixPath('examples/dataset2/session_day4.bin')}\n",
      "Tag: {'day': 'day5'}, Packet: {'bin_data': PosixPath('examples/dataset2/session_day5.bin')}\n"
     ]
    }
   ],
   "source": [
    "from orcabridge.mapper import Transform\n",
    "def transform_dataset2(tag, packet):\n",
    "    # Extract the second half of the filename containing day\n",
    "    new_tag = {'day': tag['file_name'].split('_')[1]}\n",
    "    return new_tag, packet\n",
    "\n",
    "dataset2_transformer = Transform(transform_dataset2)\n",
    "\n",
    "retagged_dataset2 = dataset2_transformer(dataset2)\n",
    "\n",
    "for tag, packet in retagged_dataset2:\n",
    "    print(f'Tag: {tag}, Packet: {packet}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have dataset2 packets tagged with `day`, let's `join`` with a mapped dataset1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag: {'day': 'day1'}, Packet: {'txt_file': PosixPath('examples/dataset1/day1.txt'), 'bin_data': PosixPath('examples/dataset2/session_day1.bin')}\n",
      "Tag: {'day': 'day3'}, Packet: {'txt_file': PosixPath('examples/dataset1/day3.txt'), 'bin_data': PosixPath('examples/dataset2/session_day3.bin')}\n",
      "Tag: {'day': 'day4'}, Packet: {'txt_file': PosixPath('examples/dataset1/day4.txt'), 'bin_data': PosixPath('examples/dataset2/session_day4.bin')}\n"
     ]
    }
   ],
   "source": [
    "# change filename to day for dataset1\n",
    "tag_mapper = MapTags(tag_map={'file_name': 'day'})\n",
    "retagged_dataset1 = tag_mapper(dataset1)\n",
    "\n",
    "join_op = Join()\n",
    "joined_stream = join_op(retagged_dataset1, retagged_dataset2)\n",
    "\n",
    "for tag, packet in joined_stream:\n",
    "    print(f'Tag: {tag}, Packet: {packet}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! We have now formed a stream where packets from two streams are paired meaningfully based on matching `day`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have explored quite a bit on how to manipulate data stream using `mapper` operations, it's time to turn to the other half ot he operations: `pods`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing new files into stream with `Pod`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While `mapper` operations are useful in altering tags, packets, and in combining multiple streams, a data pipeline is not really useful if it cannot produce new resultsin the form of new data -- that is, introduce new files into the stream. This is precisely where `Pod` operations come in!\n",
    "\n",
    "In fact, we have already been working with a `pod` all along -- `sources`. If you think about it, `sources` also introduce files into the stream. It is just special in that it takes no input streams (hence the name, `source`).\n",
    "\n",
    "We now will explore how you can create a more common type of pod -- a *function* `pod` that takes in a stream and return a new stream potentially introducing entirely new data file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with `FunctionPod`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to create a function-like `pod` is to create a `FunctionPod`, passing in a Python function. Let's start by creating a pod that will count the number of lines in a file.\n",
    "\n",
    "We first define the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import PathLike\n",
    "def count_lines(txt_file: PathLike) -> None:\n",
    "    with open(txt_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    print(f\"File {txt_file} has {len(lines)} lines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we instantiate a function pod from the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orcabridge.pod import FunctionPod\n",
    "# create a function pod\n",
    "function_pod = FunctionPod(count_lines, output_keys=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once function pod is available, you can execute it on any compatible stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File examples/dataset1/day1.txt has 24 lines.\n",
      "Tag: {'file_name': 'day1'}, Packet: {}\n",
      "File examples/dataset1/day2.txt has 15 lines.\n",
      "Tag: {'file_name': 'day2'}, Packet: {}\n",
      "File examples/dataset1/day3.txt has 27 lines.\n",
      "Tag: {'file_name': 'day3'}, Packet: {}\n",
      "File examples/dataset1/day4.txt has 22 lines.\n",
      "Tag: {'file_name': 'day4'}, Packet: {}\n"
     ]
    }
   ],
   "source": [
    "# apply the function pod on a stream\n",
    "processed_stream = function_pod(dataset1)\n",
    "\n",
    "for tag, packet in processed_stream:\n",
    "    print(f'Tag: {tag}, Packet: {packet}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the returned `packet` is empty because the function returns no values. Such a function pod may still be useful for achieving computations/processing via *side effects* (e.g., submitting HTTP requests in the function body)l, but it is not the standard approach in performing computations where you'd want the results to persis.\n",
    "\n",
    "Next, let's see how to achieve more common scenario where you perform some computation and you now would like to save the result into a file. Dataset2 binary actually contains a list of floats values. Let's define a function to compute a few statistics and save them to a file in a temporary directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tempfile\n",
    "import json\n",
    "\n",
    "def compute_stats(bin_file: PathLike, output_file=None):\n",
    "    print('Computing stats for file:', bin_file)\n",
    "    # create a temporary file to store the status and return the file path\n",
    "    with open(bin_file, 'rb') as f:\n",
    "        data = f.read()\n",
    "    data = np.frombuffer(data)\n",
    "    print(data)\n",
    "    data_stats = {}\n",
    "    data_stats['mean'] = np.mean(data)\n",
    "    data_stats['std'] = np.std(data)\n",
    "    data_stats['min'] = np.min(data)\n",
    "    data_stats['max'] = np.max(data)\n",
    "    data_stats['n_elements'] = len(data)\n",
    "\n",
    "    # if output_file is none, create a temporary file. Else, use the given output_file to save the data_stats\n",
    "    if output_file is None:\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix='.json') as temp_file:\n",
    "            output_file = temp_file.name\n",
    "    # write as json\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(data_stats, f)\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing stats for file: examples/dataset2/session_day1.bin\n",
      "[-1.08209134 -0.66806394  0.42870206 -0.09321731 -3.14078305  1.33520433\n",
      "  1.11085152  1.31931842 -1.19915697  0.07701737  1.30020807  0.27541194\n",
      "  0.84430062  0.18236837 -0.83039631 -1.66166191  0.8720775  -1.72170657\n",
      " -0.01962253 -0.18050553  1.35478472  0.69928177  0.7314272  -0.06915687\n",
      " -0.08364667 -0.45551653  0.70752188  1.02283734 -0.18612795  0.8767394\n",
      " -1.542636    1.04685484 -2.1311672  -1.34874222  0.61977577 -0.33880262\n",
      "  0.6624482   0.60257325 -3.04901544 -0.20685843 -0.08997232  0.88932232]\n",
      "Tag: {'file_name': 'session_day1'}, Packet: {'stats': '/tmp/tmpczuby3rw.json'}\n",
      "Computing stats for file: examples/dataset2/session_day3.bin\n",
      "[ 0.56114059 -1.34902274  1.0665563   0.71890802  0.65244834  1.04369548\n",
      "  0.54872876  2.19365207  0.53864286 -1.44108823 -0.55651539  0.1603561\n",
      " -0.93869224  0.64645323 -1.08815337  1.40972393 -0.14662931  1.34692375\n",
      "  0.38400938 -1.23004316  1.34426647 -0.07620065 -0.91983972  0.23537101\n",
      "  0.91515395  0.8064348   0.81470895 -1.04466683 -0.25893558 -1.46253167\n",
      "  1.39972807 -0.13940519]\n",
      "Tag: {'file_name': 'session_day3'}, Packet: {'stats': '/tmp/tmp_k0xy5q4.json'}\n",
      "Computing stats for file: examples/dataset2/session_day4.bin\n",
      "[ 0.70078854  1.18137906 -0.44361437 -0.389409    0.29719038  0.2523247\n",
      " -0.97418716  0.49301127  0.07900351 -0.29965042 -0.25810762 -2.78777445\n",
      " -1.24321702  0.13011593  1.07826637 -0.33177479 -0.78337033 -1.30075356\n",
      " -0.15710138  0.51927589  0.08671884  0.02058063  0.20778149 -1.40382559\n",
      " -0.69978105 -1.10525753  0.1945444   0.82623748  0.17467868]\n",
      "Tag: {'file_name': 'session_day4'}, Packet: {'stats': '/tmp/tmpjp7sfden.json'}\n",
      "Computing stats for file: examples/dataset2/session_day5.bin\n",
      "[ 1.9125739  -0.05252076  0.33347618  0.31627214  0.47141153 -0.71088615\n",
      " -0.74745805  0.53959117 -0.14395142 -0.28713782 -0.29422236 -1.00231383\n",
      "  0.69566576 -0.25895608 -0.9660761  -0.78504297 -1.91668262  0.89452296\n",
      " -0.82748688 -0.19792482  0.07305616  0.36133414  1.7164791   0.64364619\n",
      " -0.73146429  0.96324864 -1.05981222 -0.59502066  0.15084192]\n",
      "Tag: {'file_name': 'session_day5'}, Packet: {'stats': '/tmp/tmpw3wzwloo.json'}\n"
     ]
    }
   ],
   "source": [
    "fp_stats = FunctionPod(compute_stats, output_keys=['stats'])\n",
    "\n",
    "# change the key from 'bin_data' to 'bin_file', matching the function's input\n",
    "mapped_dataset2 = MapKeys(key_map={'bin_data': 'bin_file'})(dataset2)\n",
    "\n",
    "for tag, packet in fp_stats(mapped_dataset2):\n",
    "    print(f'Tag: {tag}, Packet: {packet}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in our function `compute_stats`, the computed stats are saved as `json` file into a temporary file. While this works to pass data from one to another within the pipeline, the result cannot be easily retrieved outside of the immediate usage. In fact, the computation result is very likely to disappear in some time (afterall, it's a temporary file). In fact, if you were to execute the same computation by iterating the second time over `stats_stream`, you will see that it invokes the functions yet again, and produces an entirely different set of temporary files. Since the content of computation didn't change, this is cearly quite wasteful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing stats for file: examples/dataset2/session_day1.bin\n",
      "[-1.08209134 -0.66806394  0.42870206 -0.09321731 -3.14078305  1.33520433\n",
      "  1.11085152  1.31931842 -1.19915697  0.07701737  1.30020807  0.27541194\n",
      "  0.84430062  0.18236837 -0.83039631 -1.66166191  0.8720775  -1.72170657\n",
      " -0.01962253 -0.18050553  1.35478472  0.69928177  0.7314272  -0.06915687\n",
      " -0.08364667 -0.45551653  0.70752188  1.02283734 -0.18612795  0.8767394\n",
      " -1.542636    1.04685484 -2.1311672  -1.34874222  0.61977577 -0.33880262\n",
      "  0.6624482   0.60257325 -3.04901544 -0.20685843 -0.08997232  0.88932232]\n",
      "Tag: {'file_name': 'session_day1'}, Packet: {'stats': '/tmp/tmpgtsizsih.json'}\n",
      "Computing stats for file: examples/dataset2/session_day3.bin\n",
      "[ 0.56114059 -1.34902274  1.0665563   0.71890802  0.65244834  1.04369548\n",
      "  0.54872876  2.19365207  0.53864286 -1.44108823 -0.55651539  0.1603561\n",
      " -0.93869224  0.64645323 -1.08815337  1.40972393 -0.14662931  1.34692375\n",
      "  0.38400938 -1.23004316  1.34426647 -0.07620065 -0.91983972  0.23537101\n",
      "  0.91515395  0.8064348   0.81470895 -1.04466683 -0.25893558 -1.46253167\n",
      "  1.39972807 -0.13940519]\n",
      "Tag: {'file_name': 'session_day3'}, Packet: {'stats': '/tmp/tmpc83yml8e.json'}\n",
      "Computing stats for file: examples/dataset2/session_day4.bin\n",
      "[ 0.70078854  1.18137906 -0.44361437 -0.389409    0.29719038  0.2523247\n",
      " -0.97418716  0.49301127  0.07900351 -0.29965042 -0.25810762 -2.78777445\n",
      " -1.24321702  0.13011593  1.07826637 -0.33177479 -0.78337033 -1.30075356\n",
      " -0.15710138  0.51927589  0.08671884  0.02058063  0.20778149 -1.40382559\n",
      " -0.69978105 -1.10525753  0.1945444   0.82623748  0.17467868]\n",
      "Tag: {'file_name': 'session_day4'}, Packet: {'stats': '/tmp/tmp8yog0nak.json'}\n",
      "Computing stats for file: examples/dataset2/session_day5.bin\n",
      "[ 1.9125739  -0.05252076  0.33347618  0.31627214  0.47141153 -0.71088615\n",
      " -0.74745805  0.53959117 -0.14395142 -0.28713782 -0.29422236 -1.00231383\n",
      "  0.69566576 -0.25895608 -0.9660761  -0.78504297 -1.91668262  0.89452296\n",
      " -0.82748688 -0.19792482  0.07305616  0.36133414  1.7164791   0.64364619\n",
      " -0.73146429  0.96324864 -1.05981222 -0.59502066  0.15084192]\n",
      "Tag: {'file_name': 'session_day5'}, Packet: {'stats': '/tmp/tmp6nd3kzrq.json'}\n"
     ]
    }
   ],
   "source": [
    "# everytime you run the following loop, new computations are performed and\n",
    "# saved in a different set of temporary files\n",
    "for tag, packet in fp_stats(mapped_dataset2):\n",
    "    print(f'Tag: {tag}, Packet: {packet}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section we will see how we can have the computation restuls stored using storage-backed function pods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Technical aside] Caching stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: This section concerns an implementation detail of `Oracbridge` that is not fundamentally related to the design of the system. In particular, the issue described in this section (and the associated *solution*) is not relevant to the full-implementation that `Orcapod` will be. If you are reading this document primarily to understand the concepts essential to Orcapod, you are advised to skip this section entirely. However, if you intend to make use of `oracabridge` in an actual application, read on to learn critical limitations associated with single-producer single-consumer (SPSC) design of the `orcabridge` and how you can ameloiorate this using `CacheStream` mapper effectively within your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing stats for file: examples/dataset2/session_day1.bin\n",
      "[-1.08209134 -0.66806394  0.42870206 -0.09321731 -3.14078305  1.33520433\n",
      "  1.11085152  1.31931842 -1.19915697  0.07701737  1.30020807  0.27541194\n",
      "  0.84430062  0.18236837 -0.83039631 -1.66166191  0.8720775  -1.72170657\n",
      " -0.01962253 -0.18050553  1.35478472  0.69928177  0.7314272  -0.06915687\n",
      " -0.08364667 -0.45551653  0.70752188  1.02283734 -0.18612795  0.8767394\n",
      " -1.542636    1.04685484 -2.1311672  -1.34874222  0.61977577 -0.33880262\n",
      "  0.6624482   0.60257325 -3.04901544 -0.20685843 -0.08997232  0.88932232]\n",
      "Tag: {'file_name': 'session_day1'}, Packet: {'stats': '/tmp/tmpbh6rd5tt.json'}\n",
      "Computing stats for file: examples/dataset2/session_day3.bin\n",
      "[ 0.56114059 -1.34902274  1.0665563   0.71890802  0.65244834  1.04369548\n",
      "  0.54872876  2.19365207  0.53864286 -1.44108823 -0.55651539  0.1603561\n",
      " -0.93869224  0.64645323 -1.08815337  1.40972393 -0.14662931  1.34692375\n",
      "  0.38400938 -1.23004316  1.34426647 -0.07620065 -0.91983972  0.23537101\n",
      "  0.91515395  0.8064348   0.81470895 -1.04466683 -0.25893558 -1.46253167\n",
      "  1.39972807 -0.13940519]\n",
      "Tag: {'file_name': 'session_day3'}, Packet: {'stats': '/tmp/tmpb9wp696s.json'}\n",
      "Computing stats for file: examples/dataset2/session_day4.bin\n",
      "[ 0.70078854  1.18137906 -0.44361437 -0.389409    0.29719038  0.2523247\n",
      " -0.97418716  0.49301127  0.07900351 -0.29965042 -0.25810762 -2.78777445\n",
      " -1.24321702  0.13011593  1.07826637 -0.33177479 -0.78337033 -1.30075356\n",
      " -0.15710138  0.51927589  0.08671884  0.02058063  0.20778149 -1.40382559\n",
      " -0.69978105 -1.10525753  0.1945444   0.82623748  0.17467868]\n",
      "Tag: {'file_name': 'session_day4'}, Packet: {'stats': '/tmp/tmpvvctvydt.json'}\n",
      "Computing stats for file: examples/dataset2/session_day5.bin\n",
      "[ 1.9125739  -0.05252076  0.33347618  0.31627214  0.47141153 -0.71088615\n",
      " -0.74745805  0.53959117 -0.14395142 -0.28713782 -0.29422236 -1.00231383\n",
      "  0.69566576 -0.25895608 -0.9660761  -0.78504297 -1.91668262  0.89452296\n",
      " -0.82748688 -0.19792482  0.07305616  0.36133414  1.7164791   0.64364619\n",
      " -0.73146429  0.96324864 -1.05981222 -0.59502066  0.15084192]\n",
      "Tag: {'file_name': 'session_day5'}, Packet: {'stats': '/tmp/tmpdte22tvv.json'}\n"
     ]
    }
   ],
   "source": [
    "from orcabridge.mapper import CacheStream\n",
    "\n",
    "# create a cache stream operation\n",
    "cache_stream = CacheStream()\n",
    "# change the key from 'bin_data' to 'bin_file', matching the function's input\n",
    "mapped_dataset2 = MapKeys(key_map={'bin_data': 'bin_file'})(dataset2)\n",
    "stats_stream = fp_stats(mapped_dataset2)\n",
    "\n",
    "# now cache the stream\n",
    "cached_stream = cache_stream(stats_stream)\n",
    "\n",
    "# iterate over the cached stream\n",
    "for tag, packet in cached_stream:\n",
    "    print(f'Tag: {tag}, Packet: {packet}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time we iterate over the `cached_stream`, you see that the function `compute_stats` is getting executed as we'd expect. However, it's when running it the second time you'd notice something is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag: {'file_name': 'session_day1'}, Packet: {'stats': '/tmp/tmpbh6rd5tt.json'}\n",
      "Tag: {'file_name': 'session_day3'}, Packet: {'stats': '/tmp/tmpb9wp696s.json'}\n",
      "Tag: {'file_name': 'session_day4'}, Packet: {'stats': '/tmp/tmpvvctvydt.json'}\n",
      "Tag: {'file_name': 'session_day5'}, Packet: {'stats': '/tmp/tmpdte22tvv.json'}\n"
     ]
    }
   ],
   "source": [
    "for tag, packet in cached_stream:\n",
    "    print(f'Tag: {tag}, Packet: {packet}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the output packets from `stats_stream` have been cached, iterating through `cached_stream` for the second time simply returned the cached packets without causing new computation. Although this may sound like a good way to prevent recomputing the same thing more than once, `CacheStream` comes with significant demerits. Since all observed packets are stored in memory, having too many `CacheStream` in the pipeline may be very memory resource heavy. Also, unlike store-backed function, as we'll see shortly, `CacheStream` stores the packets as seen from one iteration of the underlying stream. If the underlying stream would have produced new and diffirent packets (e.g., because additional `bin` files are added to the dataset), `CacheStream` won't be able to update itself without you explicitly clearing the cache. Finally, unlike storage backed function pod, computation is *not memoized* and thus same exact computation may still take place if two or more packets are identical in the content and thus would have yielded identical output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storage-backed Function Pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Function was called!\n",
      "Processing text file from examples/dataset1/day1.txt\n",
      "{'file_name': 'day1'} {'output_path': '/path/to/new/output'}\n",
      "\n",
      "\n",
      "Function was called!\n",
      "Processing text file from examples/dataset1/day2.txt\n",
      "{'file_name': 'day2'} {'output_path': '/path/to/new/output'}\n",
      "\n",
      "\n",
      "Function was called!\n",
      "Processing text file from examples/dataset1/day3.txt\n",
      "{'file_name': 'day3'} {'output_path': '/path/to/new/output'}\n",
      "\n",
      "\n",
      "Function was called!\n",
      "Processing text file from examples/dataset1/day4.txt\n",
      "{'file_name': 'day4'} {'output_path': '/path/to/new/output'}\n"
     ]
    }
   ],
   "source": [
    "from orcabridge.mapper import CacheStream\n",
    "\n",
    "# create a cache stream operation\n",
    "cache_stream = CacheStream()\n",
    "# apply the cache stream operation on a stream\n",
    "cached_stream = cache_stream(processed_stream)\n",
    "\n",
    "for tag, packet in cached_stream:\n",
    "    print(tag, packet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'table_stream2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# map second table stream to match the expected packet keys\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m mapped_stream \u001b[38;5;241m=\u001b[39m MapKeys(\u001b[43mtable_stream2\u001b[49m, key_map\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata3\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata4\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata2\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# apply the function pod on the mapped stream\u001b[39;00m\n\u001b[1;32m      5\u001b[0m processed_stream2 \u001b[38;5;241m=\u001b[39m function_pod(mapped_stream)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'table_stream2' is not defined"
     ]
    }
   ],
   "source": [
    "# map second table stream to match the expected packet keys\n",
    "mapped_stream = MapKeys(table_stream2, key_map={'data3': 'data1', 'data4': 'data2'})\n",
    "\n",
    "# apply the function pod on the mapped stream\n",
    "processed_stream2 = function_pod(mapped_stream)\n",
    "\n",
    "for tag, packet in processed_stream2:\n",
    "    print(tag, packet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that function pod operates fine as long as the incoming packet has compatible keys with the function. As tags are not passed into the function pod, the fact that above two examples had distinct types of tags does not matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using storage-backed function pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Althought the vanilla function pod `FunctionPod` illustrates the core principle of a function pod, they lack many useful features such as output storage and memoization. To use such features, you have to use a storaged-backed function pod. Here we are going to explore using DJ table-backed function pod called `DJFunctionPod`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orcabridge.dj.pod import DJFunctionPod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key difference between `DJFunctionPod` and `FunctionPod` is that the former is table-backed and therefore can store its output in a database table. For this reason, when instantiating a `DJFunctionPod`, you have to supply both a DataJoint schema and a valid table name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj_fpod = DJFunctionPod(process_data, output_keys=['output_path'], schema=schema, name='processed_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once created, you can use DJ function pod just like an ordinary function pod. However, it is keeping track and storing all computation outputs behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'animal_id': 1} {'output_path': '/path/to/new/output'}\n",
      "{'animal_id': 2} {'output_path': '/path/to/new/output'}\n",
      "{'animal_id': 3} {'output_path': '/path/to/new/output'}\n",
      "{'animal_id': 4} {'output_path': '/path/to/new/output'}\n"
     ]
    }
   ],
   "source": [
    "# apply the function pod on a stream\n",
    "processed_stream = dj_fpod(table_stream)\n",
    "\n",
    "for tag, packet in processed_stream:\n",
    "    print(tag, packet)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can appreciate the memoization if you run same computation twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'animal_id': 1} {'output_path': '/path/to/new/output'}\n",
      "{'animal_id': 2} {'output_path': '/path/to/new/output'}\n",
      "{'animal_id': 3} {'output_path': '/path/to/new/output'}\n",
      "{'animal_id': 4} {'output_path': '/path/to/new/output'}\n"
     ]
    }
   ],
   "source": [
    "# Calling the function pod on the same stream\n",
    "processed_stream = dj_fpod(table_stream)\n",
    "\n",
    "for tag, packet in processed_stream:\n",
    "    print(tag, packet)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when called the second time, function was actually **not** invoked. Rather, the stored or *memoized* result from the previous invocation was returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with regular function pod, you can run the pod on a stream with different tags (but with compatible packet keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Function was called!\n",
      "Processing data1 from /path/to/data9\n",
      "Processing data2 from /path/to/data30\n",
      "{'subject_id': 1} {'output_path': '/path/to/new/output'}\n",
      "\n",
      "\n",
      "Function was called!\n",
      "Processing data1 from /path/to/data10\n",
      "Processing data2 from /path/to/data62\n",
      "{'subject_id': 3} {'output_path': '/path/to/new/output'}\n",
      "\n",
      "\n",
      "Function was called!\n",
      "Processing data1 from /path/to/data9\n",
      "Processing data2 from /path/to/data10\n",
      "{'subject_id': 5} {'output_path': '/path/to/new/output'}\n"
     ]
    }
   ],
   "source": [
    "mapped_stream = MapKeys(table_stream2, key_map={'data3': 'data1', 'data4': 'data2'})\n",
    "\n",
    "for tag, packet in dj_fpod(mapped_stream):\n",
    "    print(tag, packet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stroage-backed function pod can serve as a stream, thus sourcing data to other operations and pods. In this case, function pod returns all stored past results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'processed_data_uuid': UUID('115b2b23-3262-278e-2b9d-350d484e68b6')} {'input_files': 'data1:/path/to/data1,data2:/path/to/data2', 'output_path': '/path/to/new/output'}\n",
      "{'processed_data_uuid': UUID('2c54462e-568a-196b-8559-65621e839c9a')} {'input_files': 'data1:/path/to/data5,data2:/path/to/data6', 'output_path': '/path/to/new/output'}\n",
      "{'processed_data_uuid': UUID('4de4316d-19d7-f92f-a6c2-010e388f39b3')} {'input_files': 'data1:/path/to/data9,data2:/path/to/data30', 'output_path': '/path/to/new/output'}\n",
      "{'processed_data_uuid': UUID('72c339d7-150c-fb7c-233e-7a8e0e030a63')} {'input_files': 'data1:/path/to/data3,data2:/path/to/data4', 'output_path': '/path/to/new/output'}\n",
      "{'processed_data_uuid': UUID('93f27a8c-0a9d-d18a-f2ca-6e4a6a7ce495')} {'input_files': 'data1:/path/to/data7,data2:/path/to/data8', 'output_path': '/path/to/new/output'}\n",
      "{'processed_data_uuid': UUID('a3a151fa-330a-1cf3-f5fa-0da62e00dd83')} {'input_files': 'data1:/path/to/data9,data2:/path/to/data10', 'output_path': '/path/to/new/output'}\n",
      "{'processed_data_uuid': UUID('a7e4bd24-ea13-8628-9111-c3b289079337')} {'input_files': 'data1:/path/to/data10,data2:/path/to/data62', 'output_path': '/path/to/new/output'}\n"
     ]
    }
   ],
   "source": [
    "for tag, packet in dj_fpod:\n",
    "    print(tag, packet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Function was called!\n",
      "Parsing data from /path/to/new/output\n",
      "{'processed_data_uuid': UUID('115b2b23-3262-278e-2b9d-350d484e68b6')} {'parsed_path': '/parsed/output/output_parsed'}\n",
      "{'processed_data_uuid': UUID('2c54462e-568a-196b-8559-65621e839c9a')} {'parsed_path': '/parsed/output/output_parsed'}\n",
      "{'processed_data_uuid': UUID('4de4316d-19d7-f92f-a6c2-010e388f39b3')} {'parsed_path': '/parsed/output/output_parsed'}\n",
      "{'processed_data_uuid': UUID('72c339d7-150c-fb7c-233e-7a8e0e030a63')} {'parsed_path': '/parsed/output/output_parsed'}\n",
      "{'processed_data_uuid': UUID('93f27a8c-0a9d-d18a-f2ca-6e4a6a7ce495')} {'parsed_path': '/parsed/output/output_parsed'}\n",
      "{'processed_data_uuid': UUID('a3a151fa-330a-1cf3-f5fa-0da62e00dd83')} {'parsed_path': '/parsed/output/output_parsed'}\n",
      "{'processed_data_uuid': UUID('a7e4bd24-ea13-8628-9111-c3b289079337')} {'parsed_path': '/parsed/output/output_parsed'}\n"
     ]
    }
   ],
   "source": [
    "def parse_data(source_data):\n",
    "    print('\\n\\nFunction was called!')\n",
    "    print(f'Parsing data from {source_data}')\n",
    "    return '/parsed/output/' + (source_data.split(\"/\")[-1]) + '_parsed'\n",
    "\n",
    "dj_fpod2 = DJFunctionPod(parse_data, output_keys=['parsed_path'], schema=schema, name='parsed_data')\n",
    "\n",
    "for tag, packet, in dj_fpod2(MapKeys(dj_fpod, key_map={'output_path': 'source_data'})):\n",
    "    print(tag, packet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth noting that pod execution are stored/memoized based on the content of the packet with no reliance on the tag. Therefore, if two or more packets with distinct tags but with identical contents are processed, only the first instance will be computed and the subsequent ones return the memoized values, saving on computation. In the above example, the function `parse_data` was called only once as all invocations involved identical packet as an input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaining together DJ-backed function pods do **not** result in foreign-key dependency to be formed between the two tables. The two data-backing tables will remain independent of each other. We will soon see how you can automatically create a DJ relational pipeline *representation* of a pipeline formed from streams, operations and pods, allowing for query to be performed using DataJoint query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"768pt\" height=\"43pt\" viewBox=\"0.00 0.00 768.00 43.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 39)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-39 764,-39 764,4 -4,4\"/>\n",
       "<!-- `enigma_orcabridge_test`.`parsed_data` -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>`enigma_orcabridge_test`.`parsed_data`</title>\n",
       "<polygon fill=\"#00ff00\" fill-opacity=\"0.188235\" stroke=\"#00ff00\" stroke-opacity=\"0.188235\" points=\"263,-35 0,-35 0,0 263,0 263,-35\"/>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-15.4\" font-family=\"arial\" text-decoration=\"underline\" font-size=\"12.00\" fill=\"darkgreen\">`enigma_orcabridge_test`.`parsed_data`</text>\n",
       "</g>\n",
       "<!-- `enigma_orcabridge_test`.`processed_data` -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>`enigma_orcabridge_test`.`processed_data`</title>\n",
       "<polygon fill=\"#00ff00\" fill-opacity=\"0.188235\" stroke=\"#00ff00\" stroke-opacity=\"0.188235\" points=\"563.5,-35 281.5,-35 281.5,0 563.5,0 563.5,-35\"/>\n",
       "<text text-anchor=\"start\" x=\"289.5\" y=\"-15.4\" font-family=\"arial\" text-decoration=\"underline\" font-size=\"12.00\" fill=\"darkgreen\">`enigma_orcabridge_test`.`processed_data`</text>\n",
       "</g>\n",
       "<!-- SampleData2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>SampleData2</title>\n",
       "<g id=\"a_node3\"><a xlink:title=\"subject_id \r------------------------------\rdata3 \rdata4 \r\">\n",
       "<polygon fill=\"#000000\" fill-opacity=\"0.125490\" stroke=\"transparent\" points=\"665,-35 582,-35 582,0 665,0 665,-35\"/>\n",
       "<text text-anchor=\"start\" x=\"590\" y=\"-16\" font-family=\"arial\" text-decoration=\"underline\" font-size=\"10.00\">SampleData2</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- SampleData -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>SampleData</title>\n",
       "<g id=\"a_node4\"><a xlink:title=\"animal_id \r------------------------------\rdata1 \rdata2 \r\">\n",
       "<polygon fill=\"#000000\" fill-opacity=\"0.125490\" stroke=\"transparent\" points=\"760,-35 683,-35 683,0 760,0 760,-35\"/>\n",
       "<text text-anchor=\"start\" x=\"691\" y=\"-16\" font-family=\"arial\" text-decoration=\"underline\" font-size=\"10.00\">SampleData</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<datajoint.diagram.Diagram at 0x7efec9332d10>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj.Diagram(schema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
