from collections import defaultdict
from collections.abc import Collection
import logging
import pickle
import sys
import time
from pathlib import Path
from typing import Any


from orcapod.core import Invocation, Kernel, SyncStream
from orcapod.core.pod import FunctionPod
from orcapod.pipeline.nodes import KernelNode, FunctionPodNode, Node

from orcapod.core.tracker import GraphTracker
from orcapod.stores import ArrowDataStore

logger = logging.getLogger(__name__)


class SerializationError(Exception):
    """Raised when pipeline cannot be serialized"""

    pass


class Pipeline(GraphTracker):
    """
    Enhanced pipeline that tracks operations and provides queryable views.
    Replaces the old Tracker with better persistence and view capabilities.
    """

    def __init__(
        self,
        name: str | tuple[str, ...],
        pipeline_store: ArrowDataStore,
        results_store: ArrowDataStore | None = None,
        auto_compile: bool = True,
    ) -> None:
        super().__init__()
        if not isinstance(name, tuple):
            name = (name,)
        self.name = name
        self.pipeline_store_path_prefix = self.name
        self.results_store_path_prefix = ()
        if results_store is None:
            if pipeline_store is None:
                raise ValueError(
                    "Either pipeline_store or results_store must be provided"
                )
            results_store = pipeline_store
            self.results_store_path_prefix = self.name + ("_results",)

        self.pipeline_store = pipeline_store
        self.results_store = results_store
        self.nodes = {}
        self.auto_compile = auto_compile
        self._dirty = False
        self._ordered_nodes = []  # Track order of invocations

    # Core Pipeline Operations
    def save(self, path: Path | str) -> None:
        """Save complete pipeline state - named functions only"""
        path = Path(path)

        # Validate serializability first
        self._validate_serializable()

        state = {
            "name": self.name,
            "invocation_lut": self.invocation_lut,
            "metadata": {
                "created_at": time.time(),
                "python_version": sys.version_info[:2],
                "orcapod_version": "0.1.0",  # TODO: make this dynamic
            },
        }

        # Atomic write
        temp_path = path.with_suffix(".tmp")
        try:
            with open(temp_path, "wb") as f:
                pickle.dump(state, f, protocol=pickle.HIGHEST_PROTOCOL)
            temp_path.replace(path)
            logger.info(f"Pipeline '{self.name}' saved to {path}")
        except Exception:
            if temp_path.exists():
                temp_path.unlink()
            raise

    def flush(self) -> None:
        """Flush all pending writes to the data store"""
        self.pipeline_store.flush()
        self.results_store.flush()
        logger.info("Pipeline stores flushed")

    def record(self, invocation: Invocation) -> None:
        """
        Record an invocation in the pipeline.
        This method is called automatically by the Kernel when an operation is invoked.
        """
        super().record(invocation)
        self._dirty = True

    def wrap_invocation(
        self, kernel: Kernel, input_nodes: Collection[Node], label: str | None = None
    ) -> Node:
        if isinstance(kernel, FunctionPod):
            return FunctionPodNode(
                kernel,
                input_nodes,
                output_store=self.results_store,
                tag_store=self.pipeline_store,
                output_store_path_prefix=self.results_store_path_prefix,
                tag_store_path_prefix=self.pipeline_store_path_prefix,
                label=label,
            )
        return KernelNode(
            kernel,
            input_nodes,
            output_store=self.pipeline_store,
            store_path_prefix=self.pipeline_store_path_prefix,
            label=label,
        )

    def compile(self):
        import networkx as nx

        G = self.generate_graph()

        # Proposed labels for each Kernel in the graph
        # If name collides, unique name is generated by appending an index
        proposed_labels = defaultdict(list)
        node_lut = {}
        edge_lut: dict[SyncStream, Node] = {}
        ordered_nodes = []
        for invocation in nx.topological_sort(G):
            # map streams to the new streams based on Nodes
            input_nodes = [edge_lut[stream] for stream in invocation.streams]
            label = None
            if invocation.has_assigned_label:
                # If the invocation has a label, use it directly
                label = invocation.label
            new_node = self.wrap_invocation(invocation.kernel, input_nodes, label=label)

            # register the new node against the original invocation
            node_lut[invocation] = new_node
            ordered_nodes.append(new_node)
            # register the new node in the proposed labels -- if duplicates occur, will resolve later
            proposed_labels[new_node.label].append(new_node)

            for edge in G.out_edges(invocation):
                edge_lut[G.edges[edge]["stream"]] = new_node

        self._ordered_nodes = ordered_nodes

        # resolve duplicates in proposed_labels
        labels_to_nodes = {}
        for label, nodes in proposed_labels.items():
            if len(nodes) > 1:
                # If multiple nodes have the same label, append index to make it unique
                for idx, node in enumerate(nodes):
                    node.label = f"{label}_{idx}"
                    labels_to_nodes[node.label] = node
            else:
                # If only one node, keep the original label
                nodes[0].label = label
                labels_to_nodes[label] = nodes[0]

        # store as pipeline's nodes attribute
        self.nodes = labels_to_nodes
        self._dirty = False
        return node_lut, edge_lut, proposed_labels, labels_to_nodes

    def __exit__(self, exc_type, exc_val, ext_tb):
        super().__exit__(exc_type, exc_val, ext_tb)
        if self.auto_compile:
            self.compile()

    def __getattr__(self, item: str) -> Any:
        """Allow direct access to pipeline attributes"""
        if item in self.nodes:
            return self.nodes[item]
        raise AttributeError(f"Pipeline has no attribute '{item}'")

    def __dir__(self):
        # Include both regular attributes and dynamic ones
        return list(super().__dir__()) + list(self.nodes.keys())

    def rename(self, old_name: str, new_name: str) -> None:
        """
        Rename a node in the pipeline.
        This will update the label and the internal mapping.
        """
        if old_name not in self.nodes:
            raise KeyError(f"Node '{old_name}' does not exist in the pipeline.")
        if new_name in self.nodes:
            raise KeyError(f"Node '{new_name}' already exists in the pipeline.")
        node = self.nodes[old_name]
        del self.nodes[old_name]
        node.label = new_name
        self.nodes[new_name] = node
        logger.info(f"Node '{old_name}' renamed to '{new_name}'")

    def run(self, full_sync: bool = False) -> None:
        """
        Run the pipeline, compiling it if necessary.
        This method is a no-op if auto_compile is False.
        """
        if self.auto_compile and self._dirty:
            self.compile()

        # Run in topological order
        for node in self._ordered_nodes:
            if full_sync:
                node.reset_cache()
            node.flow()

        self.flush()

    @classmethod
    def load(cls, path: Path | str) -> "Pipeline":
        """Load complete pipeline state"""
        path = Path(path)

        with open(path, "rb") as f:
            state = pickle.load(f)

        pipeline = cls(state["name"], state["output_store"])
        pipeline.invocation_lut = state["invocation_lut"]

        logger.info(f"Pipeline '{pipeline.name}' loaded from {path}")
        return pipeline

    def _validate_serializable(self) -> None:
        """Ensure pipeline contains only serializable operations"""
        issues = []

        for operation, invocations in self.invocation_lut.items():
            # Check for lambda functions
            if hasattr(operation, "function"):
                func = getattr(operation, "function", None)
                if func and hasattr(func, "__name__") and func.__name__ == "<lambda>":
                    issues.append(f"Lambda function in {operation.__class__.__name__}")

            # Test actual serializability
            try:
                pickle.dumps(operation)
            except Exception as e:
                issues.append(f"Non-serializable operation {operation}: {e}")

        if issues:
            raise SerializationError(
                "Pipeline contains non-serializable elements:\n"
                + "\n".join(f"  - {issue}" for issue in issues)
                + "\n\nOnly named functions are supported for serialization."
            )
