{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58c56103",
   "metadata": {},
   "source": [
    "# QoL Improving Features of `orcabridge`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16b1b64",
   "metadata": {},
   "source": [
    "In the [previous notebook](./02-02-advanced-usage.ipynb), we explored the `orcabridge` package and learned how to build and execute a simple pipeline using concepts like `streams`, `operations` and `pods`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a30ec7",
   "metadata": {},
   "source": [
    "For an example, we saw that we can define a function pod to wrap a function and to feed in a stream with the packet keys properly mapped into argument names expected by the pod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5339f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag: {'file_name': 'day1'}, Packet: {'output_data': 'path/to/result/file'}\n",
      "Tag: {'file_name': 'day2'}, Packet: {'output_data': 'path/to/result/file'}\n",
      "Tag: {'file_name': 'day3'}, Packet: {'output_data': 'path/to/result/file'}\n",
      "Tag: {'file_name': 'day4'}, Packet: {'output_data': 'path/to/result/file'}\n"
     ]
    }
   ],
   "source": [
    "from orcabridge.source import GlobSource\n",
    "from orcabridge.pod import FunctionPod\n",
    "from orcabridge.mapper import MapPackets\n",
    "\n",
    "source = GlobSource(\"text_file\", \"../examples/dataset1\", \"*.txt\")\n",
    "\n",
    "\n",
    "def process_data(data):\n",
    "    # perform data processing on data\n",
    "    # return result file path\n",
    "    return \"path/to/result/file\"\n",
    "\n",
    "\n",
    "fp_process = FunctionPod(process_data, [\"output_data\"])\n",
    "\n",
    "packet_mapper = MapPackets({\"text_file\": \"data\"})  # map packet key text_file to data\n",
    "\n",
    "# chain them together into a pipeline\n",
    "mapped_stream = packet_mapper(source)\n",
    "processed_data_stream = fp_process(mapped_stream)\n",
    "\n",
    "processed_data_stream.head()  # see the first 5 packets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b7f902",
   "metadata": {},
   "source": [
    "While separately creating all `mapper` and `pods` and then chaining them helps to rigorously define the data pipeline, admittedly it can get quite verbose and cumbersome.\n",
    "\n",
    "Fortunately, `orcabrdige` has a number of quality-of-life (QoL) improving features that will help you much more quickly create and combine `operations` and `streams` to define your pipeline without losing the full expressivity. In this notebook, we will explore such QoL improvement features together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e8c206",
   "metadata": {},
   "source": [
    "## `function_pod` decorator for simple `FunctionPod` creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f398fcfd",
   "metadata": {},
   "source": [
    "We saw that we can use `FunctionPod` class to wrap an existing function and to associate `output_keys` to rigorously define a `FunctionPod` object that can be used to perform computations on streams of data.\n",
    "\n",
    "Often, you'd want to define a function intending to only use it as a `FunctionPod`. In that case, you can simplify the `FunctionPod` creation by decorating the function with the `function_pod` decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9703d751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from orcabridge.pod import function_pod\n",
    "import json\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "json_source = GlobSource(\"json_file\", \"../examples/dataset2\", \"*.json\")\n",
    "\n",
    "\n",
    "@function_pod([\"output_data\"])\n",
    "def extract_name_from_json(json_file):\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    output_data = {\"info\": \"\"}\n",
    "    if \"name\" in data:\n",
    "        output_data[\"info\"] = data[\"name\"]\n",
    "    output_path = Path(tempfile.mkdtemp()) / \"output.json\"\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(output_data, f)\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ffe3d6",
   "metadata": {},
   "source": [
    "With the above code, the decorator takes the decorated function and creates a FunctionPod with the specified output arguments (\"output_data\" in this case).\n",
    "\n",
    "The name `extract_name_from_json` now holds the resulting `FunctionPod` that can be immeidately applied to a stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e2b5d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag: {'file_name': 'info_day1'}, Packet: {'output_data': PosixPath('/tmp/tmpn0nn30b3/output.json')}\n",
      "Tag: {'file_name': 'info_day2'}, Packet: {'output_data': PosixPath('/tmp/tmpqg13bjib/output.json')}\n",
      "Tag: {'file_name': 'info_day3'}, Packet: {'output_data': PosixPath('/tmp/tmp1f_08m5t/output.json')}\n",
      "Tag: {'file_name': 'info_day4'}, Packet: {'output_data': PosixPath('/tmp/tmpq3x8a298/output.json')}\n",
      "Tag: {'file_name': 'info_day5'}, Packet: {'output_data': PosixPath('/tmp/tmp_cma7686/output.json')}\n"
     ]
    }
   ],
   "source": [
    "extract_name_from_json(json_source).head()  # preview the first 5 packets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca2aa60",
   "metadata": {},
   "source": [
    "If you need to access the original function, it can be retrieved by accessing the `function` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5dc8a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'info': 'Day 2 experiment'}\n"
     ]
    }
   ],
   "source": [
    "output_path = extract_name_from_json.function(\"../examples/dataset2/info_day2.json\")\n",
    "\n",
    "with open(output_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    print(data)  # {'info': 'John Doe'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea0880a",
   "metadata": {},
   "source": [
    "## Mapping tags and packets with `>>` operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1e38e4",
   "metadata": {},
   "source": [
    "As you chain multiple pods together forming a complex pipeline, you are bound to make frequent use of `MapPackets` to *rename* the output argument from one pod into another argunemt name for the next pod. We have already seen how this can be achieved by creating a specific instance of `MapPackets`, initializing the object with a dictionary defining the name mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af660c4f",
   "metadata": {},
   "source": [
    "Consider the following data source and function pod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c30e1e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = GlobSource(\"json_file\", \"../examples/dataset2\", \"*.json\")\n",
    "\n",
    "\n",
    "@function_pod([\"line_count\"])\n",
    "def count_lines(text_file):\n",
    "    with open(text_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    line_count = len(lines)\n",
    "    output_path = Path(tempfile.mkdtemp()) / \"line_count.json\"\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump({\"line_count\": line_count}, f)\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6788ff05",
   "metadata": {},
   "source": [
    "If I want to apply the function pod to count and save the number of lines present in the JSON files from the data source, I will have to create a `MapPackets` that renames the output argument `\"json_file\"` itno `\"text_file\"` expected by the `count_lines` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d171b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_to_text = MapPackets(\n",
    "    {\"json_file\": \"text_file\"}\n",
    ")  # map packet key json_file to text_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7994b9b",
   "metadata": {},
   "source": [
    "Finally we can chain them together into a functional pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d48fc20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag: {'file_name': 'info_day1'}, Packet: {'line_count': PosixPath('/tmp/tmpbrelyuro/line_count.json')}\n",
      "Tag: {'file_name': 'info_day2'}, Packet: {'line_count': PosixPath('/tmp/tmp1mgotcqw/line_count.json')}\n",
      "Tag: {'file_name': 'info_day3'}, Packet: {'line_count': PosixPath('/tmp/tmp8mjlafrx/line_count.json')}\n",
      "Tag: {'file_name': 'info_day4'}, Packet: {'line_count': PosixPath('/tmp/tmpvs_r2obl/line_count.json')}\n",
      "Tag: {'file_name': 'info_day5'}, Packet: {'line_count': PosixPath('/tmp/tmpha6qrjs2/line_count.json')}\n"
     ]
    }
   ],
   "source": [
    "line_info = count_lines(json_to_text(json_files))\n",
    "\n",
    "line_info.head()  # preview the first 5 packets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d9da59",
   "metadata": {},
   "source": [
    "This is all fine until you start having many more `Pods` and `streams` in your pipeline that needs to be connected together. Many of these connection would need the `MapPackets` `mapper` to be inserted for the function to work properly -- that could be a lot of `MapPackets` you have to create!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063cc9e7",
   "metadata": {},
   "source": [
    "Because `MapPackets` is such a common operation, `orcabridge` provides a very simple shortcut for creating a *mapped stream* from another stream using right shift (`>>`) operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10437185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag: {'file_name': 'info_day1'}, Packet: {'text_file': PosixPath('../examples/dataset2/info_day1.json')}\n",
      "Tag: {'file_name': 'info_day2'}, Packet: {'text_file': PosixPath('../examples/dataset2/info_day2.json')}\n",
      "Tag: {'file_name': 'info_day3'}, Packet: {'text_file': PosixPath('../examples/dataset2/info_day3.json')}\n",
      "Tag: {'file_name': 'info_day4'}, Packet: {'text_file': PosixPath('../examples/dataset2/info_day4.json')}\n",
      "Tag: {'file_name': 'info_day5'}, Packet: {'text_file': PosixPath('../examples/dataset2/info_day5.json')}\n",
      "Tag: {'file_name': 'info_day1'}, Packet: {'line_count': PosixPath('/tmp/tmp_o8blef4/line_count.json')}\n",
      "Tag: {'file_name': 'info_day2'}, Packet: {'line_count': PosixPath('/tmp/tmp7s3mk_9p/line_count.json')}\n",
      "Tag: {'file_name': 'info_day3'}, Packet: {'line_count': PosixPath('/tmp/tmpdv672rbb/line_count.json')}\n",
      "Tag: {'file_name': 'info_day4'}, Packet: {'line_count': PosixPath('/tmp/tmpbvxkwo29/line_count.json')}\n",
      "Tag: {'file_name': 'info_day5'}, Packet: {'line_count': PosixPath('/tmp/tmp26qs4pk8/line_count.json')}\n"
     ]
    }
   ],
   "source": [
    "mapped_stream = json_files >> {\"json_file\": \"text_file\"}\n",
    "\n",
    "mapped_stream.head()\n",
    "\n",
    "count_lines(mapped_stream).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd02672",
   "metadata": {},
   "source": [
    "That's it! Hopefully you'd agree that this is far more convenient than having to define your own `MapPackets` mapper! Using the `>>` operator, the whole pipeline would have looked like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9be9f86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag: {'file_name': 'info_day1'}, Packet: {'line_count': PosixPath('/tmp/tmpcscf2auv/line_count.json')}\n",
      "Tag: {'file_name': 'info_day2'}, Packet: {'line_count': PosixPath('/tmp/tmpwtsrkvg4/line_count.json')}\n",
      "Tag: {'file_name': 'info_day3'}, Packet: {'line_count': PosixPath('/tmp/tmpw4cj_kso/line_count.json')}\n",
      "Tag: {'file_name': 'info_day4'}, Packet: {'line_count': PosixPath('/tmp/tmpyo6pc_fw/line_count.json')}\n",
      "Tag: {'file_name': 'info_day5'}, Packet: {'line_count': PosixPath('/tmp/tmp3up8exm6/line_count.json')}\n"
     ]
    }
   ],
   "source": [
    "# preview the first 5 packets\n",
    "count_lines(json_files >> {\"json_file\": \"text_file\"}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a4d11e",
   "metadata": {},
   "source": [
    "Not only is this simpler to type, we believe it actually makes the pipeline creation more intuitive and expressive of your intention!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ec64f3",
   "metadata": {},
   "source": [
    "### Mapping tags and advanced mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e8ff25",
   "metadata": {},
   "source": [
    "We just saw how the rightshift operator can be used to simplify the `MapPackets` operation creation. How about `MapTags`? We can get `MapTags` equivalent operation also by using the rightshift (`>>`) operator, but with the help of an additional function: `tag()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0164e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag: {'experiment_day': 'info_day1'}, Packet: {'json_file': PosixPath('../examples/dataset2/info_day1.json')}\n",
      "Tag: {'experiment_day': 'info_day2'}, Packet: {'json_file': PosixPath('../examples/dataset2/info_day2.json')}\n",
      "Tag: {'experiment_day': 'info_day3'}, Packet: {'json_file': PosixPath('../examples/dataset2/info_day3.json')}\n",
      "Tag: {'experiment_day': 'info_day4'}, Packet: {'json_file': PosixPath('../examples/dataset2/info_day4.json')}\n",
      "Tag: {'experiment_day': 'info_day5'}, Packet: {'json_file': PosixPath('../examples/dataset2/info_day5.json')}\n"
     ]
    }
   ],
   "source": [
    "from orcabridge.mapper import tag, packet\n",
    "\n",
    "(json_files >> tag({\"file_name\": \"experiment_day\"})).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac34eed4",
   "metadata": {},
   "source": [
    "Now if you were to closely inspect `MapPackets` and `MapPackets`, you would know that it is capable of taking in some additional arguments such as `drop_unmapped`. Using `tag()` and `packet()` helper functions would let you specify those arguments as well while using the `>>` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b366b19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag: {'file_name': 'info_day1'}, Packet: {}\n",
      "Tag: {'file_name': 'info_day2'}, Packet: {}\n",
      "Tag: {'file_name': 'info_day3'}, Packet: {}\n",
      "Tag: {'file_name': 'info_day4'}, Packet: {}\n",
      "Tag: {'file_name': 'info_day5'}, Packet: {}\n"
     ]
    }
   ],
   "source": [
    "# no packet key matches `data_file`: by default, this will lead to an empty packet\n",
    "(json_files >> {\"data_file\": \"file_path\"}).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3920086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag: {'file_name': 'info_day1'}, Packet: {'json_file': PosixPath('../examples/dataset2/info_day1.json')}\n",
      "Tag: {'file_name': 'info_day2'}, Packet: {'json_file': PosixPath('../examples/dataset2/info_day2.json')}\n",
      "Tag: {'file_name': 'info_day3'}, Packet: {'json_file': PosixPath('../examples/dataset2/info_day3.json')}\n",
      "Tag: {'file_name': 'info_day4'}, Packet: {'json_file': PosixPath('../examples/dataset2/info_day4.json')}\n",
      "Tag: {'file_name': 'info_day5'}, Packet: {'json_file': PosixPath('../examples/dataset2/info_day5.json')}\n"
     ]
    }
   ],
   "source": [
    "# you can preseve unmapped packet key by using `packet` function with `drop_unmapped=False`\n",
    "(json_files >> packet({\"data_file\": \"file_path\"}, drop_unmapped=False)).head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
